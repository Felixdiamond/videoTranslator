{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MeloTTS Setup** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:30:39.574571Z",
     "iopub.status.busy": "2025-06-01T14:30:39.574239Z",
     "iopub.status.idle": "2025-06-01T14:34:46.337217Z",
     "shell.execute_reply": "2025-06-01T14:34:46.336125Z",
     "shell.execute_reply.started": "2025-06-01T14:30:39.574537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clone MeloTTS\n",
    "!git clone https://github.com/myshell-ai/MeloTTS.git\n",
    "%cd MeloTTS\n",
    "\n",
    "# Install the package\n",
    "!pip install -e .\n",
    "\n",
    "# Download UniDic\n",
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Packages Install** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:34:46.339382Z",
     "iopub.status.busy": "2025-06-01T14:34:46.339081Z",
     "iopub.status.idle": "2025-06-01T14:35:17.358919Z",
     "shell.execute_reply": "2025-06-01T14:35:17.358058Z",
     "shell.execute_reply.started": "2025-06-01T14:34:46.339356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers accelerate\n",
    "!pip install openai-whisper\n",
    "!pip install moviepy pydub gtts\n",
    "!pip install librosa soundfile noisereduce\n",
    "!pip install psutil tqdm\n",
    "\n",
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPU Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:35:17.360140Z",
     "iopub.status.busy": "2025-06-01T14:35:17.359793Z",
     "iopub.status.idle": "2025-06-01T14:35:23.130139Z",
     "shell.execute_reply": "2025-06-01T14:35:23.129621Z",
     "shell.execute_reply.started": "2025-06-01T14:35:17.360109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# gpu_config.py\n",
    "\"\"\"\n",
    "GPU Configuration and Optimization Settings\n",
    "\"\"\"\n",
    "import torch\n",
    "import logging\n",
    "from accelerate import Accelerator\n",
    "\n",
    "class GPUOptimizer:\n",
    "    \"\"\"\n",
    "    Centralized GPU optimization configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mixed_precision='fp16', gradient_checkpointing=True):\n",
    "        try:\n",
    "            from config import config # This might raise ImportError if config.py is not in the CWD (MeloTTS/)\n",
    "            self.mixed_precision = config.get(\"gpu\", \"mixed_precision\", mixed_precision)\n",
    "            self.gradient_checkpointing = config.get(\"gpu\", \"gradient_checkpointing\", gradient_checkpointing)\n",
    "            self.compile_mode = config.get(\"gpu\", \"compile_mode\", \"reduce-overhead\")\n",
    "        except ImportError:\n",
    "            # Defaults if config.py is not found or 'config' object is not in it\n",
    "            logging.info(\"config.py not found or 'config' object missing. Using default GPUOptimizer settings.\")\n",
    "            self.mixed_precision = mixed_precision\n",
    "            self.gradient_checkpointing = gradient_checkpointing\n",
    "            self.compile_mode = \"reduce-overhead\"\n",
    "        \n",
    "        self.accelerator = None\n",
    "        self.device = None\n",
    "        self.setup_gpu()\n",
    "    \n",
    "    def setup_gpu(self):\n",
    "        \"\"\"\n",
    "        Initialize GPU settings and accelerator.\n",
    "        \"\"\"\n",
    "        # Initialize accelerator with mixed precision\n",
    "        self.accelerator = Accelerator(\n",
    "            mixed_precision=self.mixed_precision,\n",
    "            gradient_accumulation_steps=1,\n",
    "            cpu=not torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        self.device = self.accelerator.device\n",
    "        \n",
    "        # Set optimal PyTorch settings for performance\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            \n",
    "            # Print GPU information\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                gpu_props = torch.cuda.get_device_properties(i)\n",
    "                memory_gb = gpu_props.total_memory / 1024**3\n",
    "                logging.info(f\"GPU {i}: {gpu_props.name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "        logging.info(f\"Mixed precision: {self.mixed_precision}\")\n",
    "    \n",
    "    def optimize_model(self, model, compile_mode=None):\n",
    "        \"\"\"\n",
    "        Apply optimizations to a model.\n",
    "        \"\"\"\n",
    "        compile_mode_to_use = compile_mode or self.compile_mode\n",
    "        \n",
    "        # Move model to device\n",
    "        # Accelerator's prepare can handle model movement and more (like DDP setup)\n",
    "        model = self.accelerator.prepare(model)\n",
    "        \n",
    "        # Enable gradient checkpointing if supported\n",
    "        if self.gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            logging.info(\"Gradient checkpointing enabled\")\n",
    "        \n",
    "        # Apply torch.compile for 2x speed boost (PyTorch 2.0+)\n",
    "        if hasattr(torch, 'compile') and torch.cuda.is_available() and self.device.type == 'cuda':\n",
    "            logging.info(f\"Applying torch.compile with mode: {compile_mode_to_use}\")\n",
    "            try:\n",
    "                model = torch.compile(model, mode=compile_mode_to_use)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"torch.compile failed with mode {compile_mode_to_use}: {e}. Model will not be compiled.\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current GPU memory usage.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            cached = torch.cuda.memory_reserved() / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            \n",
    "            return {\n",
    "                'allocated_gb': allocated,\n",
    "                'cached_gb': cached,\n",
    "                'total_gb': total,\n",
    "                'free_gb': total - allocated # Note: free is total - allocated, not total - cached\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear GPU cache to free memory.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info(\"GPU cache cleared\")\n",
    "    \n",
    "    def log_gpu_status(self):\n",
    "        \"\"\"Log current GPU memory status.\"\"\"\n",
    "        memory_info = self.get_memory_info()\n",
    "        if memory_info:\n",
    "            logging.info(f\"GPU Memory - Allocated: {memory_info['allocated_gb']:.1f}GB, \"\n",
    "                        f\"Cached: {memory_info['cached_gb']:.1f}GB, \"\n",
    "                        f\"Free (approx): {memory_info['free_gb']:.1f}GB, \"\n",
    "                        f\"Total: {memory_info['total_gb']:.1f}GB\")\n",
    "\n",
    "# Global GPU optimizer instance\n",
    "gpu_optimizer = GPUOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performance Monitor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:35:23.132255Z",
     "iopub.status.busy": "2025-06-01T14:35:23.131967Z",
     "iopub.status.idle": "2025-06-01T14:35:23.142642Z",
     "shell.execute_reply": "2025-06-01T14:35:23.142080Z",
     "shell.execute_reply.started": "2025-06-01T14:35:23.132237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Copy and paste the entire performance_monitor.py content here\n",
    "# performance_monitor.py\n",
    "\"\"\"\n",
    "Performance monitoring and profiling utilities for video translation.\n",
    "\"\"\"\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "import functools\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor system and GPU performance during video translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    @contextmanager\n",
    "    def timer(self, operation_name: str):\n",
    "        \"\"\"Context manager to time operations.\"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = self._get_memory_usage()\n",
    "        \n",
    "        logging.info(f\"Starting {operation_name}...\")\n",
    "        gpu_optimizer.log_gpu_status() # Log GPU status at start of operation\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            end_time = time.time()\n",
    "            end_memory = self._get_memory_usage()\n",
    "            \n",
    "            duration = end_time - start_time\n",
    "            memory_delta = end_memory - start_memory if end_memory and start_memory else 0\n",
    "            \n",
    "            self.metrics[operation_name] = {\n",
    "                'duration': duration,\n",
    "                'memory_delta_mb': memory_delta,\n",
    "                'start_memory_mb': start_memory,\n",
    "                'end_memory_mb': end_memory\n",
    "            }\n",
    "            \n",
    "            logging.info(f\"Completed {operation_name} in {duration:.2f}s (Memory: {memory_delta:+.1f}MB)\")\n",
    "            gpu_optimizer.log_gpu_status() # Log GPU status at end of operation\n",
    "    \n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB.\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process()\n",
    "            return process.memory_info().rss / 1024 / 1024\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def log_gpu_status_direct(self): # Renamed to avoid conflict if gpu_optimizer is not yet fully init\n",
    "        \"\"\"Log current GPU memory status using gpu_optimizer.\"\"\"\n",
    "        gpu_optimizer.log_gpu_status()\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get performance summary.\"\"\"\n",
    "        if not self.metrics:\n",
    "            return \"No performance data collected.\"\n",
    "        \n",
    "        total_time = sum(m['duration'] for m in self.metrics.values())\n",
    "        # Peak memory delta is not just sum, but max of end_memory_mb or related metric\n",
    "        # For simplicity, current sum of deltas is kept, but it's not peak system RAM usage.\n",
    "        total_memory_change = sum(m['memory_delta_mb'] for m in self.metrics.values())\n",
    "        \n",
    "        summary = f\"\\n{'='*50}\\n\"\n",
    "        summary += \"PERFORMANCE SUMMARY\\n\"\n",
    "        summary += f\"{'='*50}\\n\"\n",
    "        summary += f\"Total Time: {total_time:.2f}s\\n\"\n",
    "        summary += f\"Net Memory Change (RAM): {total_memory_change:.1f}MB\\n\\n\"\n",
    "        \n",
    "        summary += \"Operation Breakdown:\\n\"\n",
    "        for op_name, metrics in self.metrics.items():\n",
    "            summary += f\"  {op_name}: {metrics['duration']:.2f}s \"\n",
    "            summary += f\"(RAM Î”: {metrics['memory_delta_mb']:+.1f}MB)\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Global performance monitor\n",
    "performance_monitor = PerformanceMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main translator script I: Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:35:23.143658Z",
     "iopub.status.busy": "2025-06-01T14:35:23.143372Z",
     "iopub.status.idle": "2025-06-01T14:36:27.477507Z",
     "shell.execute_reply": "2025-06-01T14:36:27.476963Z",
     "shell.execute_reply.started": "2025-06-01T14:35:23.143634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main translator script - Part 1: Imports and setup\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tempfile\n",
    "import math\n",
    "import shutil\n",
    "import mmap\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "import whisper\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "from pydub.playback import play\n",
    "from pydub.silence import detect_nonsilent\n",
    "from pydub import effects\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "# from accelerate import Accelerator # Accelerator is part of gpu_optimizer\n",
    "from melo.api import TTS\n",
    "\n",
    "# Set up logging (ensure this is run before other cells that use logging if modules were separate)\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_filepath = os.path.join(log_dir, \"video_translator.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(), \n",
    "        logging.FileHandler(log_filepath) \n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Logging system initialized.\")\n",
    "\n",
    "# Initialize melo\n",
    "global melo_tts_model\n",
    "melo_tts_model = None\n",
    "\n",
    "# Use global GPU optimizer\n",
    "accelerator = gpu_optimizer.accelerator # accelerator instance from GPUOptimizer\n",
    "device = gpu_optimizer.device         # device (cuda/cpu) from GPUOptimizer\n",
    "\n",
    "# Language mapping\n",
    "# Speaker IDs should be names that MeloTTS recognizes for the respective language, or language codes themselves if MeloTTS defaults.\n",
    "# For English, 'EN-US' is a common speaker. For others, the language code itself (e.g., 'ES', 'FR') often maps to a default speaker.\n",
    "LANGUAGE_MODEL_MAP: Dict[str, Dict[str, str]] = {\n",
    "    \"en\": {\"melo_language\": \"EN\", \"speaker_id\": \"EN-US\"}, # Changed EN-Default to EN-US\n",
    "    \"es\": {\"melo_language\": \"ES\", \"speaker_id\": \"ES\"},\n",
    "    \"fr\": {\"melo_language\": \"FR\", \"speaker_id\": \"FR\"},\n",
    "    \"zh\": {\"melo_language\": \"ZH\", \"speaker_id\": \"ZH\"},\n",
    "    \"jp\": {\"melo_language\": \"JP\", \"speaker_id\": \"JP\"},\n",
    "    \"kr\": {\"melo_language\": \"KR\", \"speaker_id\": \"KR\"},\n",
    "}\n",
    "\n",
    "DUCKING_GAIN_DB = -15\n",
    "CROSSFADE_MS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main translator script II: Core functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:36:27.479123Z",
     "iopub.status.busy": "2025-06-01T14:36:27.478365Z",
     "iopub.status.idle": "2025-06-01T14:36:27.490024Z",
     "shell.execute_reply": "2025-06-01T14:36:27.489481Z",
     "shell.execute_reply.started": "2025-06-01T14:36:27.479092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main translator script - Part 2: Core functions\n",
    "def create_project_structure(video_path: str, target_language: str) -> str:\n",
    "    print(\"Creating project structure...\")\n",
    "    base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    project_dir = os.path.join(os.getcwd(), f\"{base_name}_{target_language}_translation\")\n",
    "    os.makedirs(project_dir, exist_ok=True)\n",
    "    \n",
    "    for subdir in ['audio', 'transcripts', 'translations', 'translated_segments']:\n",
    "        os.makedirs(os.path.join(project_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    print(f\"Project directory: {project_dir}\")\n",
    "    return project_dir\n",
    "\n",
    "def memory_mapped_audio_loader(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"Load large audio files using memory mapping for efficient memory usage.\"\"\"\n",
    "    logging.info(f\"Loading audio with memory mapping: {audio_path}\")\n",
    "    \n",
    "    try:\n",
    "        with sf.SoundFile(audio_path, mode='r') as f:\n",
    "            # soundfile's read can be more memory efficient than librosa for some direct loads.\n",
    "            # However, Whisper expects a full numpy array.\n",
    "            # The main benefit here might be broader format support or faster initial I/O.\n",
    "            audio_data = f.read(dtype='float32') # Read as float32, Whisper expects this\n",
    "            if f.samplerate != 16000:\n",
    "                logging.info(f\"Resampling audio from {f.samplerate} Hz to 16000 Hz for Whisper.\")\n",
    "                # Ensure mono for resampling\n",
    "                if audio_data.ndim > 1 and audio_data.shape[1] > 1:\n",
    "                    audio_data = np.mean(audio_data, axis=1)\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=f.samplerate, target_sr=16000)\n",
    "            return audio_data\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"SoundFile loading failed: {e}, falling back to librosa.load for {audio_path}\")\n",
    "        # Librosa loads as float32 by default, and can resample.\n",
    "        audio_data, sr = librosa.load(audio_path, sr=16000, mono=True) # Whisper requires 16kHz mono\n",
    "        return audio_data\n",
    "\n",
    "\n",
    "def extract_audio(video_path: str, output_path: str) -> str:\n",
    "    logging.info(\"Extracting audio from video...\")\n",
    "    print(\"Extracting audio from video...\")\n",
    "    video = VideoFileClip(video_path)\n",
    "    # Ensure audio is written in a compatible format, e.g., WAV for consistency\n",
    "    video.audio.write_audiofile(output_path, codec='pcm_s16le', logger=None)\n",
    "    print(f\"Audio extracted to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def transcribe_with_whisper(audio_path: str) -> dict:\n",
    "    logging.info(\"Transcribing audio with Whisper...\")\n",
    "    print(\"Transcribing audio with Whisper...\")\n",
    "    \n",
    "    # Load model with GPU optimization\n",
    "    # Whisper models are relatively small, 'medium' is a good balance.\n",
    "    # device comes from gpu_optimizer\n",
    "    model = whisper.load_model(\"medium\", device=device) \n",
    "    \n",
    "    # Apply GPU optimizations (like torch.compile if applicable and enabled)\n",
    "    # Note: Whisper model might already be optimized or have parts that don't benefit as much from compile\n",
    "    model = gpu_optimizer.optimize_model(model) \n",
    "    \n",
    "    audio_data = None\n",
    "    try:\n",
    "        # Load audio as a float32 NumPy array, resampled to 16kHz mono\n",
    "        audio_data = memory_mapped_audio_loader(audio_path) \n",
    "        logging.info(f\"Audio loaded for Whisper: shape={audio_data.shape}, dtype={audio_data.dtype}\")\n",
    "        result = model.transcribe(audio_data, word_timestamps=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Whisper transcription failed: {e}\", exc_info=True)\n",
    "        # Fallback or re-raise might be needed here depending on desired robustness\n",
    "        raise\n",
    "    finally:\n",
    "        # Critical: Explicitly unload Whisper model and clear GPU cache\n",
    "        logging.info(\"Attempting to unload Whisper model and free memory.\")\n",
    "        del model \n",
    "        if audio_data is not None: \n",
    "            del audio_data \n",
    "        \n",
    "        # gc.collect() can be helpful but not a silver bullet\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        gpu_optimizer.clear_cache()\n",
    "        logging.info(\"Whisper model and associated data unloaded; GPU cache cleared.\")\n",
    "\n",
    "    print(f\"Transcription complete. Detected language: {result['language']}\")\n",
    "    logging.info(\"Whisper transcription completed.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:36:27.490984Z",
     "iopub.status.busy": "2025-06-01T14:36:27.490733Z",
     "iopub.status.idle": "2025-06-01T14:36:27.532378Z",
     "shell.execute_reply": "2025-06-01T14:36:27.531839Z",
     "shell.execute_reply.started": "2025-06-01T14:36:27.490957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_to_speech(text_to_synthesize: str, target_language: str, output_filepath: str,\n",
    "                   melo_tts_model, melo_lang_code: str, melo_spk_id_name: str, speed: float = 1.0):\n",
    "    \n",
    "    # Try MeloTTS first\n",
    "    if melo_tts_model and melo_lang_code and melo_spk_id_name:\n",
    "        try:\n",
    "            if melo_tts_model.hps.data.spk2id is None:\n",
    "                # This can happen if the model for the language doesn't have explicit speaker IDs (e.g. single speaker model)\n",
    "                # In such cases, MeloTTS might expect speaker_id=0 or handle it internally if None is passed.\n",
    "                # Let's try with a default of 0 if not found, or consult MeloTTS docs for this specific case.\n",
    "                # For now, we assume spk2id is available. If it's None, it might imply a single-speaker model\n",
    "                # where the speaker ID is implicitly 0 or not needed.\n",
    "                # However, the error indicates it *tries* to make a tensor from speaker_id.\n",
    "                # Let's check if the key exists first.\n",
    "                logging.warning(f\"MeloTTS model for language {melo_lang_code} seems to have no spk2id map. Attempting default.\")\n",
    "                # This case needs more investigation into MeloTTS internals if spk2id is None.\n",
    "                # For now, let's assume spk2id IS populated.\n",
    "                pass # This branch may need refinement if `spk2id` can be None.\n",
    "\n",
    "            if melo_spk_id_name not in melo_tts_model.hps.data.spk2id:\n",
    "                logging.error(f\"Speaker name '{melo_spk_id_name}' not found in MeloTTS model's speaker map for language {melo_lang_code}. Available: {list(melo_tts_model.hps.data.spk2id.keys())}\")\n",
    "                raise KeyError(f\"Speaker '{melo_spk_id_name}' not found.\") # Force fallback to gTTS\n",
    "\n",
    "            numerical_speaker_id = melo_tts_model.hps.data.spk2id[melo_spk_id_name]\n",
    "            \n",
    "            logging.info(f\"MeloTTS: Using lang={melo_lang_code}, spk_name='{melo_spk_id_name}', numerical_id={numerical_speaker_id}\")\n",
    "\n",
    "            melo_tts_model.tts_to_file(\n",
    "                text_to_synthesize,\n",
    "                numerical_speaker_id,  # Pass the numerical ID\n",
    "                output_filepath,\n",
    "                speed=speed\n",
    "            )\n",
    "            logging.info(f\"MeloTTS generated audio for '{text_to_synthesize[:50]}...' to {output_filepath} using lang={melo_lang_code}, spk={melo_spk_id_name}\")\n",
    "            return\n",
    "        except Exception as e: # Catching a broader exception as internal MeloTTS errors can vary\n",
    "            logging.error(f\"MeloTTS generation failed for lang='{melo_lang_code}', spk='{melo_spk_id_name}' with error: {e}. Falling back to gTTS.\")\n",
    "            import traceback\n",
    "            traceback.print_exc() \n",
    "\n",
    "    # Fallback to gTTS\n",
    "    try:\n",
    "        logging.info(f\"Using gTTS for '{text_to_synthesize[:50]}...' (lang: {target_language}) to {output_filepath}\")\n",
    "        # gTTS expects a language code like 'en', 'fr', etc.\n",
    "        tts = gTTS(text=text_to_synthesize, lang=target_language, slow=False)\n",
    "        tts.save(output_filepath)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"gTTS generation failed for '{text_to_synthesize[:50]}...' with error: {e}. Skipping segment audio.\")\n",
    "        # Create a very short silent file to prevent downstream errors if no audio is generated\n",
    "        AudioSegment.silent(duration=10, frame_rate=22050).export(output_filepath, format=\"wav\")\n",
    "\n",
    "def _translate_text_internal(text: str, source_language: str, target_language: str, translation_model, translation_tokenizer) -> str:\n",
    "    logging.info(f\"Translating chunk of text from {source_language} to {target_language}...\")\n",
    "    if not text.strip(): # Handle empty input text\n",
    "        logging.info(\"Input text for translation is empty. Returning empty string.\")\n",
    "        return \"\"\n",
    "    if source_language == target_language:\n",
    "        return text\n",
    "\n",
    "    # Ensure model and tokenizer are on the correct device (handled by accelerator.prepare generally)\n",
    "    # inputs = translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    # .to(device) might be redundant if model is already prepared and on device, tokenizer usually follows.\n",
    "    raw_inputs = translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in raw_inputs.items()} # Explicitly move to device\n",
    "\n",
    "    # For mBART-like models that require src_lang to be set on tokenizer\n",
    "    if hasattr(translation_tokenizer, 'src_lang') and source_language:\n",
    "        # mBART uses specific language codes (e.g., 'en_XX', 'fr_XX')\n",
    "        # We need a mapping if `source_language` is 'en', 'fr'\n",
    "        # For now, assume `source_language` is already in the correct format if mBART is used, or Helsinki model doesn't need it.\n",
    "        # This might need refinement if mixing models with different lang code requirements.\n",
    "        translation_tokenizer.src_lang = source_language \n",
    "    \n",
    "    forced_bos_token_id = None\n",
    "    if hasattr(translation_tokenizer, 'lang_code_to_id') and target_language in translation_tokenizer.lang_code_to_id:\n",
    "        forced_bos_token_id = translation_tokenizer.lang_code_to_id[target_language]\n",
    "    elif hasattr(translation_tokenizer, 'get_lang_id') and callable(translation_tokenizer.get_lang_id):\n",
    "        try:\n",
    "            forced_bos_token_id = translation_tokenizer.get_lang_id(target_language)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not get lang_id for {target_language} from tokenizer: {e}\")\n",
    "\n",
    "\n",
    "    with autocast(enabled=(accelerator.mixed_precision == 'fp16' or accelerator.mixed_precision == 'bf16')):\n",
    "        if forced_bos_token_id is not None:\n",
    "            translated_tokens = translation_model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_new_tokens=512)\n",
    "        else:\n",
    "            # This path is for models that don't use/need forced_bos_token_id (e.g. some Helsinki models for specific pairs)\n",
    "            translated_tokens = translation_model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    translated_text = translation_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def _summarize_text_internal(text: str, summarizer_pipeline, max_length_ratio: float = 0.75, min_length_abs: int = 30) -> str:\n",
    "    print(\"Summarizing text...\")\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    text_len_chars = len(text)\n",
    "    # Heuristic: if text is already short, don't summarize or summarize less aggressively\n",
    "    if text_len_chars < 100: # Adjust this threshold as needed\n",
    "        # Summarization might not be effective or necessary for very short texts\n",
    "        # For now, we try to summarize anyway, but one could return text as is.\n",
    "        pass \n",
    "\n",
    "    # Calculate max_length based on ratio, ensuring it's not too small\n",
    "    # Summarization models often work better with token counts, but char count is an approximation\n",
    "    # Max length should generally be less than the input length.\n",
    "    calculated_max_length = int(text_len_chars * max_length_ratio)\n",
    "    # Ensure max_length is at least min_length_abs + a small margin, and less than text_len_chars\n",
    "    final_max_length = max(min_length_abs + 10, calculated_max_length) \n",
    "    final_max_length = min(final_max_length, text_len_chars -1) # Ensure it's shorter than original\n",
    "    \n",
    "    # Ensure min_length is less than max_length and not excessively small\n",
    "    final_min_length = min(min_length_abs, final_max_length - 5)\n",
    "    final_min_length = max(10, final_min_length) # Absolute minimum length\n",
    "\n",
    "    if final_min_length >= final_max_length:\n",
    "        logging.warning(f\"Cannot summarize: min_length ({final_min_length}) >= max_length ({final_max_length}). Returning original text.\")\n",
    "        return text\n",
    "    \n",
    "    logging.info(f\"Summarizing with min_length={final_min_length}, max_length={final_max_length}\")\n",
    "\n",
    "    with autocast(enabled=(accelerator.mixed_precision == 'fp16' or accelerator.mixed_precision == 'bf16')):\n",
    "        try:\n",
    "            summary = summarizer_pipeline(text, max_length=final_max_length, min_length=final_min_length, do_sample=False)[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Summarization failed: {e}. Returning original text.\")\n",
    "            return text\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio processing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:36:27.533703Z",
     "iopub.status.busy": "2025-06-01T14:36:27.533132Z",
     "iopub.status.idle": "2025-06-01T14:36:27.560897Z",
     "shell.execute_reply": "2025-06-01T14:36:27.560368Z",
     "shell.execute_reply.started": "2025-06-01T14:36:27.533678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Audio processing functions\n",
    "def noise_reduction(audio: AudioSegment, reduction_amount=0.8):\n",
    "    \"\"\"Apply noise reduction to the audio segment. reduction_amount (0.0 to 1.0)\"\"\"\n",
    "    if not (0.0 <= reduction_amount <= 1.0):\n",
    "        logging.warning(f\"Noise reduction_amount {reduction_amount} out of bounds [0,1]. Clamping.\")\n",
    "        reduction_amount = max(0.0, min(1.0, reduction_amount))\n",
    "\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
    "    # Ensure samples are float for noisereduce, and normalize if they are not in [-1, 1] range\n",
    "    # pydub samples are integers, so convert to float and scale\n",
    "    if audio.sample_width == 2: # 16-bit\n",
    "        samples = samples / (2**15) \n",
    "    elif audio.sample_width == 1: # 8-bit\n",
    "        samples = (samples - 128) / 128.0\n",
    "    # Add other sample widths if necessary\n",
    "\n",
    "    # Noisereduce expects mono audio. If stereo, process channels separately or convert to mono.\n",
    "    # For simplicity, let's assume mono or average stereo for noise profile.\n",
    "    if audio.channels == 2:\n",
    "        samples_mono_for_noise_profile = samples.reshape((-1, 2)).mean(axis=1)\n",
    "        noise_clip = nr.reduce_noise(y=samples_mono_for_noise_profile, sr=audio.frame_rate, prop_decrease=0, n_fft=2048, hop_length=512, y_noise=None, stationary=False) # Get noise profile\n",
    "        \n",
    "        reduced_noise_channels = []\n",
    "        for i in range(audio.channels):\n",
    "            channel_samples = samples.reshape((-1, audio.channels))[:, i]\n",
    "            reduced_channel = nr.reduce_noise(y=channel_samples, sr=audio.frame_rate, y_noise=noise_clip, prop_decrease=reduction_amount, stationary=True)\n",
    "            reduced_noise_channels.append(reduced_channel)\n",
    "        reduced_noise_samples = np.stack(reduced_noise_channels, axis=-1).flatten()\n",
    "    else: # Mono\n",
    "        reduced_noise_samples = nr.reduce_noise(y=samples, sr=audio.frame_rate, prop_decrease=reduction_amount)\n",
    "    \n",
    "    # Convert back to original integer type for AudioSegment\n",
    "    if audio.sample_width == 2:\n",
    "        reduced_noise_samples = (reduced_noise_samples * (2**15)).astype(np.int16)\n",
    "    elif audio.sample_width == 1:\n",
    "        reduced_noise_samples = ((reduced_noise_samples * 128) + 128).astype(np.uint8)\n",
    "\n",
    "    return AudioSegment(\n",
    "        reduced_noise_samples.tobytes(),\n",
    "        frame_rate=audio.frame_rate,\n",
    "        sample_width=audio.sample_width,\n",
    "        channels=audio.channels\n",
    "    )\n",
    "\n",
    "def enhance_voice(audio: AudioSegment):\n",
    "    \"\"\"Enhance the voice in the audio segment.\"\"\"\n",
    "    # Apply a high-pass filter to remove low-frequency rumble\n",
    "    enhanced = audio.high_pass_filter(80)\n",
    "    # Apply a low-pass filter to remove high-frequency hiss (optional, depends on source)\n",
    "    # enhanced = enhanced.low_pass_filter(12000) # Adjust frequency as needed\n",
    "    \n",
    "    # Compressor to even out volume levels\n",
    "    # Threshold: -20dBFS, Ratio: 4:1, Attack: 5ms, Release: 50ms\n",
    "    # These are common starting points, may need tuning.\n",
    "    enhanced = effects.compress_dynamic_range(enhanced, threshold=-20.0, ratio=4.0, attack=5.0, release=50.0)\n",
    "    \n",
    "    # Normalize to a target level (e.g., -3dBFS) to ensure consistent loudness\n",
    "    enhanced = effects.normalize(enhanced, headroom=3.0)\n",
    "    return enhanced\n",
    "\n",
    "def advanced_time_stretch(audio: AudioSegment, target_duration: int) -> AudioSegment:\n",
    "    if len(audio) == 0 or target_duration <=0:\n",
    "        logging.warning(f\"Cannot time stretch, invalid input audio length {len(audio)} or target duration {target_duration}.\")\n",
    "        return audio if len(audio) > 0 else AudioSegment.silent(duration=10)\n",
    "    \n",
    "    logging.info(f\"Performing advanced time stretch. Input: {len(audio)/1000:.2f}s, Target: {target_duration/1000:.2f}s\")\n",
    "    \n",
    "    # Convert pydub AudioSegment to numpy array (float32, normalized to [-1, 1])\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
    "    if audio.sample_width == 2:\n",
    "        samples /= (2**15) # For 16-bit audio\n",
    "    elif audio.sample_width == 1: # 8-bit unsigned\n",
    "        samples = (samples - 128) / 128.0\n",
    "    # Add other sample widths if necessary\n",
    "\n",
    "    if audio.channels == 2:\n",
    "        samples_mono = samples.reshape((-1, 2)).mean(axis=1) # Librosa time_stretch needs mono\n",
    "    else:\n",
    "        samples_mono = samples\n",
    "    \n",
    "    stretch_factor = len(audio) / target_duration # rate for librosa.effects.time_stretch\n",
    "    # Clamp stretch_factor to avoid extreme distortion (e.g., 0.5x to 2.0x speed)\n",
    "    # rate < 1 slows down, rate > 1 speeds up.\n",
    "    # So if current is 10s, target is 5s, factor is 2 (speed up by 2x)\n",
    "    # if current is 5s, target is 10s, factor is 0.5 (slow down by 2x)\n",
    "    stretch_factor = max(min(stretch_factor, 2.0), 0.5)\n",
    "    \n",
    "    logging.info(f\"Adjusted stretch factor (rate for librosa): {stretch_factor:.2f}\")\n",
    "    \n",
    "    # Librosa time_stretch\n",
    "    stretched_samples_mono = librosa.effects.time_stretch(samples_mono, rate=stretch_factor)\n",
    "    \n",
    "    # If original was stereo, we need to create a stereo output from stretched mono\n",
    "    # A simple way is to duplicate the mono channel.\n",
    "    if audio.channels == 2:\n",
    "        stretched_samples = np.vstack((stretched_samples_mono, stretched_samples_mono)).T.flatten()\n",
    "    else:\n",
    "        stretched_samples = stretched_samples_mono\n",
    "\n",
    "    # Ensure correct duration by padding or truncating (librosa stretch is approximate)\n",
    "    target_num_samples = int(target_duration / 1000 * audio.frame_rate * audio.channels)\n",
    "    current_num_samples = len(stretched_samples)\n",
    "\n",
    "    if current_num_samples > target_num_samples:\n",
    "        stretched_samples = stretched_samples[:target_num_samples]\n",
    "    elif current_num_samples < target_num_samples:\n",
    "        padding = np.zeros(target_num_samples - current_num_samples)\n",
    "        stretched_samples = np.concatenate([stretched_samples, padding])\n",
    "    \n",
    "    # Convert back to original integer type for AudioSegment\n",
    "    if audio.sample_width == 2:\n",
    "        stretched_samples = (stretched_samples * (2**15)).astype(np.int16)\n",
    "    elif audio.sample_width == 1:\n",
    "        stretched_samples = ((stretched_samples * 128) + 128).astype(np.uint8)\n",
    "    \n",
    "    stretched_audio = AudioSegment(\n",
    "        stretched_samples.tobytes(),\n",
    "        frame_rate=audio.frame_rate,\n",
    "        sample_width=audio.sample_width,\n",
    "        channels=audio.channels\n",
    "    )\n",
    "    \n",
    "    return stretched_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segment Processing and sync**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:36:27.561773Z",
     "iopub.status.busy": "2025-06-01T14:36:27.561554Z",
     "iopub.status.idle": "2025-06-01T14:36:27.590227Z",
     "shell.execute_reply": "2025-06-01T14:36:27.589740Z",
     "shell.execute_reply.started": "2025-06-01T14:36:27.561749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# In your main script, replace the existing process_segment function\n",
    "def process_segment(segment_info: Tuple[int, dict], original_audio_segment_duration: int, \n",
    "                    source_language: str, target_language: str, segments_dir: str,\n",
    "                    translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model):\n",
    "    i, segment = segment_info # Unpack index and segment data\n",
    "    start_time_ms = int(segment['start'] * 1000)\n",
    "    end_time_ms = int(segment['end'] * 1000)\n",
    "    # original_duration_ms = end_time_ms - start_time_ms # This is duration of original text segment\n",
    "    # We use original_audio_segment_duration which is passed if it's more accurate (e.g. from audio split)\n",
    "    # For now, let's use the segment's own duration. If Whisper segments are reliable, this is fine.\n",
    "    original_duration_ms = end_time_ms - start_time_ms\n",
    "    if original_duration_ms <= 0: # Avoid issues with zero/negative duration segments\n",
    "        logging.warning(f\"Segment {i} has zero or negative duration. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Processing segment {i}: '{segment['text'][:50]}...' ({original_duration_ms}ms)\")\n",
    "\n",
    "    # Translate segment text\n",
    "    translated_segment_text = _translate_text_internal(segment['text'], source_language, target_language, translation_model, translation_tokenizer)\n",
    "    \n",
    "    if not translated_segment_text.strip():\n",
    "        logging.warning(f\"Skipping empty translated segment for: '{segment['text']}'\")\n",
    "        return None\n",
    "    \n",
    "    tts_output_path = os.path.join(segments_dir, f\"segment_{i:04d}.wav\")\n",
    "    \n",
    "    try:\n",
    "        melo_lang_info = LANGUAGE_MODEL_MAP.get(target_language)\n",
    "        if melo_lang_info:\n",
    "            melo_lang_code = melo_lang_info[\"melo_language\"]\n",
    "            melo_spk_id_name = melo_lang_info[\"speaker_id\"] if melo_lang_info else target_language.upper()\n",
    "        else:\n",
    "            logging.warning(f\"MeloTTS language/speaker info not found for {target_language}. Fallback to gTTS will occur.\")\n",
    "            melo_lang_code = target_language # Best guess for MeloTTS if specific code not mapped\n",
    "            melo_spk_id_name = target_language # Best guess for MeloTTS speaker if not mapped\n",
    "\n",
    "        # Generate initial TTS audio\n",
    "        text_to_speech(\n",
    "            text_to_synthesize=translated_segment_text, # CORRECTED: was 'translated_segment'\n",
    "            target_language=target_language, # For gTTS fallback\n",
    "            output_filepath=tts_output_path,\n",
    "            melo_tts_model=melo_tts_model,\n",
    "            melo_lang_code=melo_lang_code, # For MeloTTS\n",
    "            melo_spk_id_name=melo_spk_id_name, # For MeloTTS\n",
    "            speed=1.0 # MeloTTS base speed\n",
    "        )\n",
    "            \n",
    "        translated_audio_segment = AudioSegment.from_file(tts_output_path)\n",
    "        current_tts_duration = len(translated_audio_segment)\n",
    "\n",
    "        # Summarization if TTS audio is much longer than original segment's duration\n",
    "        # Threshold: e.g., if TTS is 50% longer than original, or more than 3s longer absolute\n",
    "        # (len(translated_audio_segment) - original_duration_ms >= 3000)\n",
    "        if current_tts_duration > original_duration_ms * 1.5 and current_tts_duration - original_duration_ms > 1500:\n",
    "            logging.info(f\"Segment {i} TTS ({current_tts_duration}ms) is much longer than original ({original_duration_ms}ms). Summarizing text.\")\n",
    "            summarized_text = _summarize_text_internal(translated_segment_text, summarizer_pipeline)\n",
    "            if summarized_text.strip() and len(summarized_text) < len(translated_segment_text):\n",
    "                text_to_speech(\n",
    "                    text_to_synthesize=summarized_text,\n",
    "                    target_language=target_language,\n",
    "                    output_filepath=tts_output_path,\n",
    "                    melo_tts_model=melo_tts_model,\n",
    "                    melo_lang_code=melo_lang_code,\n",
    "                    melo_spk_id_name=melo_spk_id_name,\n",
    "                    speed=1.0\n",
    "                )\n",
    "                translated_audio_segment = AudioSegment.from_file(tts_output_path)\n",
    "                logging.info(f\"Segment {i} re-synthesized with summarized text. New TTS duration: {len(translated_audio_segment)}ms\")\n",
    "            else:\n",
    "                logging.info(f\"Summarization did not significantly shorten text for segment {i}. Using original TTS.\")\n",
    "        \n",
    "        # Audio processing pipeline\n",
    "        # 1. Time Stretch to match original segment duration\n",
    "        stretched_audio = advanced_time_stretch(translated_audio_segment, original_duration_ms)\n",
    "        del translated_audio_segment; import gc; gc.collect()\n",
    "        \n",
    "        # 2. Noise Reduction (optional, can be light)\n",
    "        # noise_reduced_audio = noise_reduction(stretched_audio, reduction_amount=0.1) # Light reduction\n",
    "        # del stretched_audio; import gc; gc.collect()\n",
    "        # Current noise_reduction implementation needs float samples, ensure advanced_time_stretch provides compatible output or adjust here\n",
    "        # For now, skipping noise reduction here as it can be sensitive.\n",
    "        processed_audio = stretched_audio # If skipping NR\n",
    "\n",
    "        # 3. Voice Enhancement (Normalization, Light Compression, EQ)\n",
    "        enhanced_audio = enhance_voice(processed_audio)\n",
    "        if processed_audio != enhanced_audio: # only del if it's a new object\n",
    "             del processed_audio; import gc; gc.collect()\n",
    "        \n",
    "        # Export the final processed segment audio to disk\n",
    "        enhanced_audio.export(tts_output_path, format=\"wav\")\n",
    "        del enhanced_audio; import gc; gc.collect()\n",
    "        \n",
    "        return (start_time_ms, tts_output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing segment {i} ('{segment['text'][:30]}...'): {str(e)}\", exc_info=True)\n",
    "        gpu_optimizer.clear_cache() # Clear cache on error too\n",
    "        import gc; gc.collect()\n",
    "        return None\n",
    "\n",
    "def adaptive_segment_processing(segments_data: List[dict], original_audio: AudioSegment, \n",
    "                                source_language: str, target_language: str, project_dir: str,\n",
    "                                translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model):\n",
    "    segments_dir = os.path.join(project_dir, 'translated_segments')\n",
    "    os.makedirs(segments_dir, exist_ok=True)\n",
    "    \n",
    "    processed_segments_info = []\n",
    "    # Process sequentially for now to ensure stability, especially with GPU memory.\n",
    "    # The original_audio object isn't directly used per segment here, but segment timings are from transcript.\n",
    "    # If segment durations need to be derived from audio splits, that logic would be different.\n",
    "    \n",
    "    tasks = []\n",
    "    for i, segment in enumerate(segments_data):\n",
    "        # We pass the duration of the original segment based on its timestamps.\n",
    "        # If Whisper segments are very precise, this is fine.\n",
    "        # original_segment_duration_ms = int((segment['end'] - segment['start']) * 1000)\n",
    "        # The process_segment function will calculate this from segment['start'] and segment['end'] itself.\n",
    "        tasks.append(((i, segment), 0, source_language, target_language, segments_dir,\n",
    "                      translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model))\n",
    "\n",
    "    for task_args in tqdm(tasks, desc=\"Processing segments\"):\n",
    "        # Unpack to match new signature of process_segment\n",
    "        segment_info, _, src_lang, tgt_lang, seg_dir, trans_model, trans_tok, summ_pipe, melo_model = task_args\n",
    "        # The 'original_audio_segment_duration' is not explicitly passed here, \n",
    "        # process_segment calculates it from segment timestamps.\n",
    "        result = process_segment(segment_info, 0, src_lang, tgt_lang, seg_dir,\n",
    "                                 trans_model, trans_tok, summ_pipe, melo_model)\n",
    "        if result:\n",
    "            processed_segments_info.append(result)\n",
    "        # Optional: aggressive cleanup after each segment if memory is extremely tight\n",
    "        # gpu_optimizer.clear_cache()\n",
    "        # import gc; gc.collect()\n",
    "            \n",
    "    return processed_segments_info\n",
    "\n",
    "def preserve_sound_effects(original_audio: AudioSegment, \n",
    "                           synced_speech: AudioSegment, \n",
    "                           transcript: dict, \n",
    "                           project_dir: str, \n",
    "                           silence_original_speech: bool = True,\n",
    "                           duck_gain_for_background: Optional[float] = DUCKING_GAIN_DB) -> AudioSegment:\n",
    "    \n",
    "    debug_audio_dir = os.path.join(project_dir, 'audio_debug')\n",
    "    os.makedirs(debug_audio_dir, exist_ok=True)\n",
    "\n",
    "    logging.info(f\"Mixing audio. Silence original speech: {silence_original_speech}. Background duck gain: {duck_gain_for_background}dB\")\n",
    "    \n",
    "    target_channels = original_audio.channels # Default to original's channels\n",
    "    if original_audio.channels == 1 and synced_speech.channels == 2:\n",
    "        target_channels = 2 # If original is mono but TTS is stereo, prefer stereo\n",
    "    \n",
    "    working_original_audio = original_audio\n",
    "    if working_original_audio.channels != target_channels:\n",
    "        logging.info(f\"Standardizing 'working_original_audio' from {working_original_audio.channels} to {target_channels} channels.\")\n",
    "        working_original_audio = working_original_audio.set_channels(target_channels)\n",
    "    \n",
    "    if synced_speech.channels != target_channels:\n",
    "        logging.info(f\"Standardizing 'synced_speech' from {synced_speech.channels} to {target_channels} channels.\")\n",
    "        synced_speech = synced_speech.set_channels(target_channels)\n",
    "\n",
    "    # Save initial states for debugging\n",
    "    working_original_audio.export(os.path.join(debug_audio_dir, \"0_working_original_audio_standardized.wav\"), format=\"wav\")\n",
    "    synced_speech.export(os.path.join(debug_audio_dir, \"0_synced_french_speech_standardized.wav\"), format=\"wav\")\n",
    "\n",
    "    # This will become the original audio track with speech sections silenced or ducked\n",
    "    processed_original_audio = AudioSegment.empty() # Start with an empty segment\n",
    "\n",
    "    if silence_original_speech:\n",
    "        logging.info(\"Reconstructing original audio with speech segments silenced.\")\n",
    "        last_end_ms = 0\n",
    "        speech_intervals_ms = sorted([(int(s['start']*1000), int(s['end']*1000)) for s in transcript['segments'] if int(s['end']*1000) > int(s['start']*1000)])\n",
    "\n",
    "        for start_ms, end_ms in speech_intervals_ms:\n",
    "            # Append non-speech part from original\n",
    "            if start_ms > last_end_ms:\n",
    "                non_speech_part = working_original_audio[last_end_ms:start_ms]\n",
    "                processed_original_audio += non_speech_part\n",
    "            \n",
    "            # Append silence for the speech part\n",
    "            duration_ms = end_ms - start_ms\n",
    "            if duration_ms > 0:\n",
    "                silence = AudioSegment.silent(duration=duration_ms, frame_rate=working_original_audio.frame_rate)\n",
    "                silence = silence.set_channels(working_original_audio.channels)\n",
    "                processed_original_audio += silence\n",
    "            last_end_ms = end_ms\n",
    "        \n",
    "        # Append any remaining part of the original audio after the last speech segment\n",
    "        if last_end_ms < len(working_original_audio):\n",
    "            remaining_part = working_original_audio[last_end_ms:]\n",
    "            processed_original_audio += remaining_part\n",
    "        \n",
    "        # Ensure the processed_original_audio has the correct total duration, pad with silence if necessary\n",
    "        if len(processed_original_audio) < len(working_original_audio):\n",
    "            padding_duration = len(working_original_audio) - len(processed_original_audio)\n",
    "            padding_silence = AudioSegment.silent(duration=padding_duration, frame_rate=working_original_audio.frame_rate).set_channels(working_original_audio.channels)\n",
    "            processed_original_audio += padding_silence\n",
    "        elif len(processed_original_audio) > len(working_original_audio):\n",
    "            processed_original_audio = processed_original_audio[:len(working_original_audio)]\n",
    "\n",
    "\n",
    "        logging.info(f\"Reconstructed original audio with silenced speech. Duration: {len(processed_original_audio)/1000}s\")\n",
    "        processed_original_audio.export(os.path.join(debug_audio_dir, \"1_processed_original_AFTER_silencing.wav\"), format=\"wav\")\n",
    "\n",
    "    elif duck_gain_for_background is not None: \n",
    "        logging.info(f\"Applying ducking with gain {duck_gain_for_background}dB to original speech segments.\")\n",
    "        processed_original_audio = working_original_audio.dup() # Start with a copy for ducking\n",
    "        speech_intervals_ms = [(int(s['start']*1000), int(s['end']*1000)) for s in transcript['segments'] if int(s['end']*1000) > int(s['start']*1000)]\n",
    "        for start_ms, end_ms in speech_intervals_ms:\n",
    "            duck_eff_start = max(0, start_ms - CROSSFADE_MS) \n",
    "            duck_eff_end = min(len(processed_original_audio), end_ms + CROSSFADE_MS)\n",
    "            if duck_eff_start < duck_eff_end :\n",
    "                segment_to_duck = processed_original_audio[duck_eff_start:duck_eff_end]\n",
    "                ducked_segment = segment_to_duck.apply_gain(duck_gain_for_background) \n",
    "                processed_original_audio = processed_original_audio.overlay(ducked_segment, position=duck_eff_start)\n",
    "        logging.info(f\"Applied ducking. Duration: {len(processed_original_audio)/1000}s\")\n",
    "        processed_original_audio.export(os.path.join(debug_audio_dir, \"1_processed_original_AFTER_ducking.wav\"), format=\"wav\")\n",
    "    else:\n",
    "        # Neither silencing nor ducking, just use the standardized original\n",
    "        processed_original_audio = working_original_audio\n",
    "        logging.info(\"No silencing or ducking applied to original audio.\")\n",
    "        processed_original_audio.export(os.path.join(debug_audio_dir, \"1_processed_original_NO_OP.wav\"), format=\"wav\")\n",
    "\n",
    "\n",
    "    logging.info(f\"Overlaying translated speech (duration: {len(synced_speech)/1000}s) onto processed original (duration: {len(processed_original_audio)/1000}s)...\")\n",
    "    \n",
    "    # Ensure channel counts match perfectly before the final overlay\n",
    "    if processed_original_audio.channels != synced_speech.channels:\n",
    "        logging.error(f\"FINAL ATTEMPT CHANNEL MISMATCH. Processed Original: {processed_original_audio.channels}, Synced Speech: {synced_speech.channels}. This should not happen if standardization worked.\")\n",
    "        # Force one to match the other, e.g. make synced_speech match processed_original_audio\n",
    "        synced_speech = synced_speech.set_channels(processed_original_audio.channels)\n",
    "        \n",
    "    final_audio_mix = processed_original_audio.overlay(synced_speech)\n",
    "    final_audio_mix.export(os.path.join(debug_audio_dir, \"2_final_mixed_audio.wav\"), format=\"wav\")\n",
    "    \n",
    "    logging.info(f\"Audio mixing complete. Final duration: {len(final_audio_mix)/1000}s\")\n",
    "    return final_audio_mix\n",
    "\n",
    "def create_synced_audio(original_audio: AudioSegment, transcript: dict, source_language: str, target_language: str, project_dir: str,\n",
    "                        translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model) -> AudioSegment:\n",
    "    logging.info(\"Creating synced translated audio with segment fades...\")\n",
    "\n",
    "    final_synced_speech_track = AudioSegment.silent(\n",
    "        duration=len(original_audio),\n",
    "        frame_rate=original_audio.frame_rate\n",
    "    ).set_channels(original_audio.channels)\n",
    "\n",
    "    # Process all segments (generates individual .wav files for each translated segment)\n",
    "    processed_segments_info = adaptive_segment_processing(\n",
    "        transcript['segments'], original_audio, \n",
    "        source_language, target_language, project_dir,\n",
    "        translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model\n",
    "    )\n",
    "\n",
    "    # Load each processed segment, apply fades, and overlay onto the main translated speech track\n",
    "    for start_time_ms, audio_filepath in processed_segments_info:\n",
    "        try:\n",
    "            segment_audio = AudioSegment.from_file(audio_filepath)\n",
    "\n",
    "            if CROSSFADE_MS > 0 and len(segment_audio) > CROSSFADE_MS * 2:\n",
    "                segment_audio = segment_audio.fade_in(CROSSFADE_MS).fade_out(CROSSFADE_MS)\n",
    "            elif CROSSFADE_MS > 0 and len(segment_audio) > 0 : # Shorter segments, shorter fade\n",
    "                 fade_len = min(CROSSFADE_MS, len(segment_audio)//2)\n",
    "                 if fade_len > 0 : segment_audio = segment_audio.fade_in(fade_len).fade_out(fade_len)\n",
    "\n",
    "            final_synced_speech_track = final_synced_speech_track.overlay(segment_audio, position=start_time_ms)\n",
    "\n",
    "            del segment_audio\n",
    "            import gc; gc.collect()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error overlaying segment from {audio_filepath}: {str(e)}\", exc_info=True)\n",
    "\n",
    "    return final_synced_speech_track\n",
    "\n",
    "def create_final_video(video_path: str, final_audio_path: str, output_path: str):\n",
    "    logging.info(\"Creating final video...\")\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = AudioFileClip(final_audio_path)\n",
    "    final_clip = video.set_audio(audio)\n",
    "    # Specify threads for moviepy for potentially faster writing, and logger for verbosity.\n",
    "    final_clip.write_videofile(output_path, audio_codec='aac', threads=4, logger='bar')\n",
    "    del video, audio, final_clip; import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process Video function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T14:36:27.592362Z",
     "iopub.status.busy": "2025-06-01T14:36:27.591823Z",
     "iopub.status.idle": "2025-06-01T14:36:27.612367Z",
     "shell.execute_reply": "2025-06-01T14:36:27.611803Z",
     "shell.execute_reply.started": "2025-06-01T14:36:27.592345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# In your main script, replace the existing process_video function\n",
    "def process_video(video_path: str, target_language: str):\n",
    "    performance_monitor.log_gpu_status_direct() # Use direct to avoid issues if monitor's own gpu_optimizer not ready\n",
    "    \n",
    "    with performance_monitor.timer(\"total_processing\"):\n",
    "        project_dir = create_project_structure(video_path, target_language)\n",
    "        \n",
    "        translation_model = None\n",
    "        translation_tokenizer = None\n",
    "        summarizer_pipeline = None\n",
    "        # melo_tts_model is global, will be loaded once.\n",
    "        global melo_tts_model\n",
    "\n",
    "        try:\n",
    "            with performance_monitor.timer(\"audio_extraction\"):\n",
    "                audio_path = os.path.join(project_dir, 'audio', 'extracted_audio.wav')\n",
    "                extract_audio(video_path, audio_path)\n",
    "            \n",
    "            with performance_monitor.timer(\"transcription\"):\n",
    "                transcript_result = transcribe_with_whisper(audio_path) \n",
    "                source_language = transcript_result['language']\n",
    "                # Ensure source_language is a short code like 'en', 'fr'\n",
    "                if source_language and len(source_language) > 2 and '-' in source_language:\n",
    "                    source_language = source_language.split('-')[0]\n",
    "                logging.info(f\"Detected source language: {source_language}\")\n",
    "                print(f\"Source: {source_language}, Target: {target_language}\")\n",
    "            \n",
    "            transcript_path = os.path.join(project_dir, 'transcripts', 'transcript.txt')\n",
    "            with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(transcript_result['text'])\n",
    "            \n",
    "            # --- LOAD MODELS (ONCE PER TYPE) ---\n",
    "            with performance_monitor.timer(\"translation_model_loading\"):\n",
    "                # Prioritize Helsinki-NLP if source_language is available\n",
    "                # Note: source_language from Whisper might be 'en', 'fr', etc.\n",
    "                # Helsinki models are usually like 'opus-mt-en-fr'\n",
    "                # mBART requires lang codes like 'en_XX', 'fr_XX'\n",
    "                # This needs careful handling based on what Whisper returns vs what models expect.\n",
    "                \n",
    "                # Assuming Whisper's source_language (e.g., 'en') is okay for Helsinki naming\n",
    "                specific_model_name = f\"Helsinki-NLP/opus-mt-{source_language}-{target_language}\"\n",
    "                try:\n",
    "                    logging.info(f\"Attempting to load translation model: {specific_model_name}\")\n",
    "                    translation_tokenizer = AutoTokenizer.from_pretrained(specific_model_name)\n",
    "                    translation_model = AutoModelForSeq2SeqLM.from_pretrained(specific_model_name)\n",
    "                    translation_model = gpu_optimizer.optimize_model(translation_model)\n",
    "                    logging.info(f\"Successfully loaded and optimized {specific_model_name}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to load {specific_model_name} ({e}). Falling back to mBART.\")\n",
    "                    if translation_model: del translation_model\n",
    "                    if translation_tokenizer: del translation_tokenizer\n",
    "                    gpu_optimizer.clear_cache(); import gc; gc.collect()\n",
    "\n",
    "                    mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "                    logging.info(f\"Loading mBART model: {mbart_model_name}\")\n",
    "                    # For mBART, source_language needs to be in xx_YY format, e.g. 'en_XX'\n",
    "                    # This requires a mapping if Whisper gives 'en'. For now, we'll assume User ensures correct input or tokenizer handles it.\n",
    "                    # For simplicity, if Whisper gives 'en', mBART might need 'en_XX'.\n",
    "                    # The _translate_text_internal function will need to handle tokenizer.src_lang correctly.\n",
    "                    translation_tokenizer = AutoTokenizer.from_pretrained(mbart_model_name, src_lang=f\"{source_language}_XX\" if len(source_language)==2 else source_language) # Adjust src_lang format for mBART\n",
    "                    translation_model = AutoModelForSeq2SeqLM.from_pretrained(mbart_model_name)\n",
    "                    translation_model = gpu_optimizer.optimize_model(translation_model)\n",
    "                    logging.info(f\"Successfully loaded and optimized {mbart_model_name}\")\n",
    "            \n",
    "            with performance_monitor.timer(\"summarization_model_loading\"):\n",
    "                # BART for summarization\n",
    "                summarizer_model_name = \"facebook/bart-large-cnn\"\n",
    "                logging.info(f\"Loading summarization pipeline with model: {summarizer_model_name}\")\n",
    "                # device=-1 for CPU, or gpu_optimizer.device.index for specific GPU if using Transformers pipeline device arg\n",
    "                summarizer_device_arg = device.index if device.type == 'cuda' else -1 \n",
    "                summarizer_pipeline = pipeline(\"summarization\", model=summarizer_model_name, device=summarizer_device_arg, framework=\"pt\")\n",
    "                # Optionally compile the model inside the pipeline if not done by optimize_model\n",
    "                # if hasattr(summarizer_pipeline.model, 'parameters') and device.type == 'cuda':\n",
    "                #    summarizer_pipeline.model = gpu_optimizer.optimize_model(summarizer_pipeline.model)\n",
    "                logging.info(\"Summarization pipeline loaded.\")\n",
    "\n",
    "            with performance_monitor.timer(\"melo_tts_model_loading\"):\n",
    "                if melo_tts_model is None: # Load only if not already loaded\n",
    "                    try:\n",
    "                        # MeloTTS uses 'EN', 'FR' etc. for language codes. Use the one from LANGUAGE_MODEL_MAP.\n",
    "                        # Initial load language doesn't matter as much since tts_to_file specifies language.\n",
    "                        initial_melo_lang = LANGUAGE_MODEL_MAP.get(target_language, {}).get(\"melo_language\", \"EN\")\n",
    "                        logging.info(f\"Loading MeloTTS model globally (initial lang: {initial_melo_lang}, device: {device})...\")\n",
    "                        melo_tts_model = TTS(language=initial_melo_lang, device=str(device)) # device expects 'cpu' or 'cuda:0'\n",
    "                        logging.info(\"MeloTTS model loaded globally.\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Failed to load MeloTTS model globally: {e}. TTS will rely on gTTS.\", exc_info=True)\n",
    "                        melo_tts_model = None # Ensure it's None if loading failed\n",
    "            \n",
    "            # Translate the entire transcript text (for saving and potential full-text use)\n",
    "            with performance_monitor.timer(\"full_text_translation\"):\n",
    "                # This full translation is for the .txt output. Segments are re-translated for TTS generation.\n",
    "                full_translated_text = _translate_text_internal(transcript_result['text'], source_language, target_language, translation_model, translation_tokenizer)\n",
    "            \n",
    "            translation_path = os.path.join(project_dir, 'translations', 'translation.txt')\n",
    "            with open(translation_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_translated_text)\n",
    "            \n",
    "            with performance_monitor.timer(\"audio_synthesis_and_sync\"):\n",
    "                original_audio = AudioSegment.from_wav(audio_path)\n",
    "                logging.info(f\"Original audio duration: {len(original_audio)/1000:.2f}s\")\n",
    "                \n",
    "                # Pass loaded models to create_synced_audio\n",
    "                synced_translated_speech = create_synced_audio(\n",
    "                    original_audio, transcript_result, \n",
    "                    source_language, target_language, project_dir,\n",
    "                    translation_model, translation_tokenizer, summarizer_pipeline, melo_tts_model\n",
    "                )\n",
    "            \n",
    "            with performance_monitor.timer(\"audio_mixing_with_effects\"):\n",
    "                final_mixed_audio = preserve_sound_effects(\n",
    "                    original_audio, \n",
    "                    synced_translated_speech, \n",
    "                    transcript_result, \n",
    "                    project_dir, \n",
    "                    silence_original_speech=True\n",
    "                ) \n",
    "                del original_audio, synced_translated_speech; import gc; gc.collect()\n",
    "            \n",
    "            final_audio_path = os.path.join(project_dir, 'audio', 'final_audio.wav')\n",
    "            logging.info(f\"Exporting final mixed audio to: {final_audio_path}\")\n",
    "            final_mixed_audio.export(final_audio_path, format=\"wav\")\n",
    "            del final_mixed_audio; import gc; gc.collect()\n",
    "            \n",
    "            with performance_monitor.timer(\"final_video_creation\"):\n",
    "                output_video_path = os.path.join(project_dir, f\"translated_{os.path.basename(video_path)}\")\n",
    "                create_final_video(video_path, final_audio_path, output_video_path)\n",
    "            \n",
    "            logging.info(f\"Translation complete. Output video: {output_video_path}\")\n",
    "            logging.info(performance_monitor.get_summary())\n",
    "            return output_video_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during video processing: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "        finally:\n",
    "            # Cleanup models regardless of success or failure\n",
    "            logging.info(\"Cleaning up models...\")\n",
    "            if translation_model: del translation_model\n",
    "            if translation_tokenizer: del translation_tokenizer\n",
    "            if summarizer_pipeline:\n",
    "                 if hasattr(summarizer_pipeline, 'model'): del summarizer_pipeline.model # Try to delete inner model\n",
    "                 del summarizer_pipeline\n",
    "            # Global melo_tts_model is kept for potential subsequent runs in notebook, \n",
    "            # but if this were a script, it would be deleted here too.\n",
    "            # For notebook, explicit clearing may be desired by user if kernel restarted.\n",
    "            gpu_optimizer.clear_cache()\n",
    "            import gc; gc.collect()\n",
    "            logging.info(\"Cleanup complete.\")\n",
    "\n",
    "print(\"âœ… All functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run the translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:05:56.078775Z",
     "iopub.status.busy": "2025-06-01T15:05:56.077526Z",
     "iopub.status.idle": "2025-06-01T15:17:21.373590Z",
     "shell.execute_reply": "2025-06-01T15:17:21.372831Z",
     "shell.execute_reply.started": "2025-06-01T15:05:56.078746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run the translation\n",
    "target_language = \"fr\"  # Change this to your desired language (en, es, fr, jp, kr, zh)\n",
    "video_filename = \"/kaggle/input/source-vids/fern_eng_short.mp4\" # Ensure this path is correct for your Kaggle environment\n",
    "\n",
    "# Ensure MeloTTS is ready (especially if running cells individually)\n",
    "if melo_tts_model is None and 'process_video' in globals():\n",
    "    logging.info(\"MeloTTS model is not loaded. Attempting to load before main processing.\")\n",
    "    try:\n",
    "        # Simplified MeloTTS loading for pre-check - actual load is in process_video\n",
    "        # This is more of a conceptual check; process_video handles the real loading.\n",
    "        initial_melo_lang = LANGUAGE_MODEL_MAP.get(target_language, {}).get(\"melo_language\", \"EN\")\n",
    "        # melo_tts_model = TTS(language=initial_melo_lang, device=str(gpu_optimizer.device))\n",
    "        # logging.info(f\"Pre-checked MeloTTS model loading for {initial_melo_lang}.\")\n",
    "        # Actually, process_video will handle this. This is just a note.\n",
    "        pass \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pre-check: Failed to load MeloTTS model: {e}\")\n",
    "\n",
    "\n",
    "# Run translation\n",
    "try:\n",
    "    output_video = process_video(video_filename, target_language)\n",
    "    print(f\"\\nðŸŽ‰ Translation completed!\")\n",
    "    print(f\"ðŸ“ Output video: {output_video}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during translation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12546699,
     "datasetId": 7564531,
     "sourceId": 12026189,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
